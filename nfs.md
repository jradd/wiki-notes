# NFS
Misc NFS

### RPC

##### Duplicate Request Cache
Used for removing non-idempotent rpc calls, as to not attempt to `rm` a file
more than once.

- xid
Transaction ID:
Unique key generated by client and lives in the request header of each entry
used by server to store and identify duplicate requests.

- DNLC:
Directory Name Lookup Cache:
Used in kernel to prevent every lookup request from going to
an NFS server. ??? Is this in client or server ???


### File Handles


#### Stale
File handles become stale, when the inodes to which they point (on the server)
are freed or re-used.

If one client removes a file and then creates a new file that re-uses the freed
inode, other FH's  (on other clients) that point to the re-used inode must
be marked stale.

Inode generation numbers were added to the basic Unix filesystem to add a time
history to an inode.

In addition to the inode number , the fh must match the current generation
number of the inode, or it is marked stale. When the inode is re-used for a new
file, its generation number is incremented. 

Can become a problem if server is rebuilt from backup tape. 
A good way to cripple NFS network is to restore a fileserver from a 
tape backup without rebooting the clients.

When you rebuild the server's filesystems, all of the inode generation numbers
are reset; when you load the tape, files end up with a different
generation numbers than they had on the original filesystem.

All nfs FH are now invalid.

MUST UNMOUNT CLIENTS IF PLANNING TO RESTORE NFS SERVER, OR REBOOT CLIENTS!

- vnodes:
client vfs points to vnode on local fs
nfs vnode points to a structure containing an NFS filehandle.



### TCP
* Solaris - maintains single tcp connection for all mount points  

* BSD - Single connection per mount point  


### nfsd

Daemons or threads service requests in a pseudo-round robing
fasion whenever a daemon or thread is done with a request it goes to the end of
the queue waiting for a new request.

Using this scheduling algorithm, a server is always able
to accept a new NFS request as long as at least one daemon or thread is waiting
in queue.

Running multiple daemons or threads lets a server start multiple disk operations
at the same time and handle quick turnaround requests such as getattr and lookup
while disk-bound requests
 are in progress.

nfsd takes care to inform the kernel of new connections, and connection closures


- buffer size:
used for disk i/o requests is independent of the networks MTU and the server or 
client filesystemblock size. Is chosen by the most efficient size handled by the
network transport protocol, and is usually **8K for nfsv2**, **32k for nfs v3**

writes are batched until a full nfs buffer has been written.


### Troubleshoot

#### Client
`mount -vvv -t nfs` 
`cat /proc/self/mountstats`
`cat /proc/net/rpc/*/connect`

`rpcinfo -s` 
`lsb_release -sir; rpm -q nfs-utils` 

`rpcinfo -p |grep -w mountd |grep -w tcp`


#### Server

#### Ramblings
Hard coding the FSID's on the server side can help stale mount problems
when changing exports frequently.


Having a low latency network essential

NFS does not support 'delete on last close' semantics


The numbers after dev= are in hexadecimal. Device numbers are constructed by
taking the major number, shifting it left several bits, and then adding the 
minor number. Convert the minor number 1823 to hexadecimal, and look for it in
the mount table:

```
# client
printf "%x\n" 1823
  71f
# client
mount | grep 'dev=.*71f'
    /home/mre on spike:/export/home/mre
    remote/read/write/setuid/intr/dev=340071f
```




